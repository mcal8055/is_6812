---
title: "EDA"
author: "Josh McAlister"
output: 
   html_document:
    theme: cosmo
    toc: true
    toc_depth: 2
    toc_float: true
    highlight: tango
    embed-resources: true   
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
```
# Executive Summary

Home Credit, a multinational provider of financial services to non-traditional or unbanked clients, is currently unnecessarily rejecting creditworthy potential customers. This represents missed revenue for the company and provides a poor experience to prospective borrowers. A variety of data sources are available for determining creditworthiness, allowing Home Credit to extend loans to those traditionally considered under and unbanked. However, the lack of complete credit data for individual customers sourced from financial institutions proves a challenge in producing NA values when joined to the main dataset. In fact, the presence of NAs generated by many Home Credit's applicants' lack of participation in traditional credit products represents one of the larger challenges when selecting predictive variables.  

#Initial Exploration

```{r initial exploration}
#read in data
train_data <- read.csv("application_train.csv")

#inspect source file and structure
if (interactive()) {
  glimpse(train_data)
  str(train_data)
}

#standardize variable names
train_data <- train_data |>
  rename_with(tolower)

#transform target variable
train_data <- train_data |>
  mutate(target = factor(target, levels = c(0, 1), labels = c("on-time", "late")))

#convert variables to factor where appropriate
train_data <- train_data |>
  mutate(name_education_type = factor(name_education_type), 
         name_family_status = factor(name_family_status))

#majority class accuracy 
train_data |>
  count(target, name = "n") |>
  mutate(maj_acc = round(n / sum(n)), 2)

#explore NA cells

#totals
sum(is.na(train_data))

#by column
colSums(is.na(train_data))

#identify placeholder value for days_employed
train_data |>
  summarise(max = max(days_employed))

#summary stats for select variables
summary_stats <- train_data |>
  summarise(
    #financials
    income = median(amt_income_total), 
    income_sd = sd(amt_income_total), 
    credit = median(amt_credit), 
    credit_sd = sd(amt_credit),
    loan_annuity = median(amt_annuity, na.rm = TRUE),
    sd_loan_annuity = sd(amt_annuity, na.rm = TRUE),
    
    #demographics
    region_population = median(region_population_relative, na.rm = TRUE), 
    sd_population = sd(region_population_relative, na.rm = TRUE),
    family_size = median(cnt_fam_members, na.rm = TRUE), 
    sd_family_size = sd(cnt_fam_members, na.rm = TRUE), 
    thirty_days_past_due_community = median(def_30_cnt_social_circle, na.rm = TRUE), 
    sd_thirty_days_past_due_community = sd(def_30_cnt_social_circle, na.rm = TRUE), 
    sixty_days_past_due_community = median(def_60_cnt_social_circle, na.rm = TRUE), 
    sd_sixty_days_past_due_community = sd(def_60_cnt_social_circle, na.rm = TRUE),
    
    #credit history
    credit_pull_3mo = median(amt_req_credit_bureau_qrt, na.rm = TRUE),
    sd_credit_pull_3mo = sd(amt_req_credit_bureau_qrt, na.rm = TRUE),
    credit_pull_12mo = median(amt_req_credit_bureau_year, na.rm = TRUE),
    sd_credit_pull_12mo = sd(amt_req_credit_bureau_year, na.rm = TRUE))

pull(summary_stats)
```
-Simple baseline majority accuracy reveals a significant class imbalance - how will we account for this when evaluating models?
-Many key demographic and financial history variables contain NA values, either because an answer was not given or no data exists - how should this be handled?
  i. Instead of removing NAs and potentially missing valuable signals, perhaps 0-encoding these values and handling with a log transformation could be the key?

# Selected plots of variables from train_data

```{r exploratory plot of train_data, warning=FALSE}
#boxplot of repayment trouble vs income before log transformation
ggplot(data = train_data, mapping = aes(x = target, y = amt_income_total)) + 
  geom_boxplot() + 
  labs(title = "Late Payment Status ~ Income")

#income distribution
ggplot(data = train_data, mapping = aes(x = amt_income_total)) +
  geom_histogram() +
  labs(title = "Income Distribution")

#explore income distribution outliers
library(scales)
quantile(train_data$amt_income_total, c(0,.25,.5,.75,.9,.95,.99,.999,1), na.rm = TRUE)

#apply log transformation for distribution analysis
ggplot(subset(train_data, amt_income_total > 0), aes(amt_income_total)) +
  geom_histogram(bins = 80) +
  scale_x_log10(labels = scales::label_number(big.mark = ",")) +
  labs(title = "Income â€” log10 x-axis", x = "amt_income_total (log10)")

#reevaluate boxplot after log transformation
brks  <- c(5e4, 1e5, 2e5, 5e5, 1e6)  # choose any readable values
ggplot(train_data, aes(x = target, y = log10(amt_income_total))) +
  geom_boxplot(outlier.alpha = 0.25, varwidth = TRUE) +
  scale_y_continuous(
    breaks = log10(brks),
    labels = label_number(scale_cut = cut_si(" "))(brks)) +
  labs(title = "Income by target computed in log space",
       y = "amt_income_total (log10)")

#boxplot of repayment trouble vs loan size
ggplot(data = train_data, mapping = aes(x = target, y = amt_credit)) + 
  geom_boxplot() + 
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Late Payment Status ~ Credit")

#boxplot of repayment trouble vs employment tenure
df <- train_data |>
  mutate(
    unemployed_flag   = days_employed == 365243,
    days_emp_clean    = ifelse(unemployed_flag, NA, days_employed),
    years_emp         = -days_emp_clean / 365)

# Boxplot without the sentinel
ggplot(df, aes(target, years_emp)) +
  geom_boxplot(outlier.shape = NA, varwidth = TRUE) +
  labs(title = "Employment tenure (years) by target")

# Share of sentinel group by target
ggplot(df, aes(target, fill = unemployed_flag)) +
  geom_bar(position = "fill") +
  labs(title = "Unemployed/unknown share by target", y = "Share", fill = "Sentinel")

# Faceted boxplots
cont_vars <- c("amt_credit", "amt_annuity",
               "days_birth", "amt_req_credit_bureau_year", "amt_req_credit_bureau_qrt",
               "def_60_cnt_social_circle")

long_box <- train_data %>%
  select(target, all_of(cont_vars)) %>%
  pivot_longer(cols = -target, names_to = "variable", values_to = "value")

ggplot(long_box, aes(x = target, y = value)) +
  geom_boxplot(outlier.alpha = 0.2, varwidth = TRUE) +
  facet_wrap(~ variable, scales = "free_y", ncol = 3) +
  labs(title = "Continuous variables ~ target", x = NULL, y = NULL)

#barplot of repayment trouble vs home ownership
ggplot(data = train_data, mapping = aes(x = target, fill = flag_own_realty)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent)
  labs(title = "Late Payment Status ~ Home Ownership", y = "Share", fill = "Owns Property")

#barplot of repayment trouble vs car ownership
ggplot(data = train_data, mapping = aes(x = target, fill = flag_own_car)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent)
  labs(title = "Late Payment Status ~ Car Ownership", y = "Share", fill = "Owns Auto")
  
#repayment trouble vs education level
ggplot(train_data, aes(x = name_education_type, fill = target)) +
  geom_bar(position = "fill") +
  labs(title = "Late-payment share by education",
       x = "Education", y = "Share") +
  coord_flip()

#repayment trouble vs 30 DPD in social surroundings
ggplot(train_data |>
         mutate(nonzero = def_30_cnt_social_circle > 0),
       aes(target, fill = nonzero)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Any 30+ Days Past Due in social circle?", y = "Share", fill = "> 0")

#repayment trouble vs 60+ DPD in social surroundings
ggplot(train_data |>
         mutate(nonzero = def_60_cnt_social_circle > 0),
       aes(target, fill = nonzero)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) +
  labs(title = "Any 60+ Days Past Due in social circle?", y = "Share", fill = "> 0")

```

-Simple baseline majority accuracy reveals a significant class imbalance - how will we account for this when evaluating models?
-Initial boxplot of payment status (target variable) vs income reveals the influence of outliers. Log scaling is useful for this data. 
  i. Income appears to have predictive power 
-Home ownership does not not appear to have a strong marginal association with late payments
-Car ownership appears to have a stronger association with late payments
-Age appears to have a strong association with late payments
-days_employed contains placeholder code value of 365243, flag and replace with NA value for analysis
  i. Count of placeholder values are not insignificant - how to handle?
  ii. Assuming removal of placeholders, employment tenure appears to have association with late payments
-A higher count of 30 & 60 days past due loans in applicant's social circle shows an association with payment trouble
  i. Data is zero-inflated, collapsed box plot reveals
  ii. Instances of DPD are comparatively rare, but likely still offer predictive power
  iii. Given that Home Credit strives to lend credit to un/underbanked clients, social equivalents could potentially serve as a very useful predictor
  
# Join with bureau data  
  
```{r prepare joined data}
#read in csv
bureau_data <- read.csv("bureau.csv")

#lowercase transformation
bureau_data <- bureau_data |>
  rename_with(tolower)

#recode as factor as necessary
bureau_data <- bureau_data |>
  mutate(credit_active = as.factor(credit_active), 
         credit_type = as.factor(credit_type))

#examine factor levels
summary(bureau_data$credit_type)

summary(bureau_data$credit_active)

#aggregate data
agg_var <- bureau_data |>
  group_by(sk_id_curr) |>
  summarise(
    
    #status aggregates
    n_lines = n(), 
    n_active = sum(credit_active == "Active", na.rm = TRUE), 
    n_bad_debt = sum(credit_active == "Bad debt", na.rm = TRUE), 
    n_closed = sum(credit_active == "Closed", na.rm = TRUE), 
    n_sold = sum(credit_active == "Sold", na.rm = TRUE),
    
    #delinquency aggregates
    n_overdue = sum(credit_day_overdue > 0, na.rm = TRUE), 
    sum_overdue = sum(amt_credit_sum_overdue, na.rm = TRUE), 
    max_dpd = max(credit_day_overdue), 
    sum_debt = sum(amt_credit_sum_debt, na.rm = TRUE),
    max_overdue = max(amt_credit_max_overdue),
    
    #recency aggregates
    oldest_open_days = min(days_credit),
    newest_open_days = max(days_credit),
    
    #credit type aggregates
    n_mortgage = sum(credit_type == "Mortgage", na.rm = TRUE),
    n_auto = sum(credit_type == "Car loan", na.rm = TRUE),
    n_consumer_credit = sum(credit_type == "Consumer credit", na.rm = TRUE),
    n_credit_card = sum(credit_type == "Credit card", na.rm = TRUE),
    n_microloan = sum(credit_type == "Microloan", na.rm = TRUE),
    .groups = "drop"
)

#join to train_data on sk_id_curr and add no bureau data flag
train_join <- train_data |>
  left_join(agg_var, by = "sk_id_curr") |>
  mutate(no_bureau = is.na(n_lines))

```

# Selected plots of joined datasets  

```{r explore/visualize select joined data}
#explore NA cells
#totals
sum(is.na(train_join))

#by column
colSums(is.na(train_join))

#Plots

#drop applicants with no bureau rows for bureau-dependent plots
nb_data <- train_join |> 
  filter(!no_bureau)

# Recency aggregates
days_vars  <- c("oldest_open_days","newest_open_days")

#convert to year format
days_long <- nb_data |>
  select(target, any_of(days_vars)) |>
  pivot_longer(cols = -target, names_to = "variable", values_to = "value") |>
  mutate(years_since = -value/365)

ggplot(days_long, aes(target, years_since)) +
  geom_boxplot(outlier.shape = NA, varwidth = TRUE) +
  facet_wrap(~ variable, ncol = 2, scales = "free_y") +
  labs(title = "Default ~ Credit History in Years", x = NULL, y = "Years")
```

# Results Summary

As might be expected, many of the traditional indicators of creditworthiness do not appear to have any association with the late payments/defaults of Home Credit's borrowers. To better predict this outcome, we will have to look outside the margins of traditional creditworthiness. Demographic data and financial history should prove to have predictive power, but non-response in loan application data results in a large number of NA values for key variables. Elimination of those NAs from the dataset could prove costly as we might miss predictive signals. The data also demonstrates default is a rare outcome, we will need to account for this class imbalance when selecting and evaluating predictive models. 
